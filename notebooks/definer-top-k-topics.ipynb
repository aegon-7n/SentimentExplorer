{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aegon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/aegon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "from pyyoutube import Api\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import f1_score, silhouette_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pymorphy2\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = logging.getLogger(__name__)\n",
    "# logger.setLevel(logging.INFO)\n",
    "\n",
    "# handler = logging.FileHandler('../logging/logging.log')\n",
    "# logger.addHandler(handler)\n",
    "\n",
    "# formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "# handler.setFormatter(formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(YOUTUBE_API_KEY, video_id, max_results, next_page_token):\n",
    "    youtube_uri = f'https://www.googleapis.com/youtube/v3/commentThreads?key={YOUTUBE_API_KEY}&textFormat=plainText&' + \\\n",
    "        f'part=snippet&videoId={video_id}&maxResults={max_results}&pageToken={next_page_token}'\n",
    "    \n",
    "    try: \n",
    "        response = requests.get(youtube_uri)\n",
    "        response.raise_for_status()\n",
    "        data = json.loads(response.text)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_text_of_comment(data):\n",
    "    comms = set()\n",
    "    for item in data['items']:\n",
    "        comm = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "        comms.add(comm)\n",
    "    return comms\n",
    "\n",
    "\n",
    "def get_all_comments(YOUTUBE_API_KEY, query, count_video=10, limit=30, max_results=10, next_page_token=''):\n",
    "    api = Api(api_key=YOUTUBE_API_KEY)\n",
    "    video_by_keywords = api.search_by_keywords(q=query,\n",
    "                                               search_type=[\"video\"],\n",
    "                                               count=count_video,\n",
    "                                               limit=limit\n",
    "    )\n",
    "    video_ids = [x.id.videoId for x in video_by_keywords.items]\n",
    "\n",
    "    comments_all = []\n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            data = get_data(YOUTUBE_API_KEY,\n",
    "                            video_id=video_id,\n",
    "                            max_results=max_results,\n",
    "                            next_page_token=next_page_token\n",
    "            )\n",
    "            \n",
    "            if 'items' in data:\n",
    "                comment = list(get_text_of_comment(data))\n",
    "                comments_all.append(comment)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    comments = sum(comments_all, [])\n",
    "    return comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'affinity': 'cosine',\n",
       " 'count_max_clusters': 15,\n",
       " 'silhouette_metric': 'euclidean'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_path = os.path.join('/home/aegon/Documents/Development/Projects/topic-sentiment-explorer/config/params_all.yaml')\n",
    "config = yaml.safe_load(open(config_path))['train']\n",
    "config['clustering']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–®–∏–∫–æ—Å, –±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –≤–∏–¥–µ–æ!!\\n–í—Å–µ–º –¥–æ–±—Ä–∞)',\n",
       " '–ù–µ–ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–º—É —Å–ª—É—à–∞—Ç–µ–ª—é –≤–æ–æ–±—â–µ –Ω–µ–ø–æ–Ω—è—Ç–Ω–æ. \"–ù–∞ –≤—ã—Ö–æ–¥–µ –≤–µ–∫—Ç–æ—Ä...\"',\n",
       " '–ü–æ–¥—Å–∫–∞–∂–∏—Ç–µ –ø–æ–∂–∞–ª—É–π—Å—Ç–∞ –∫–∞–∫–æ–π —Ç–∏–ø –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–µ—Ç–∏ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ Deep Fake –º–µ–¥–∏–∞',\n",
       " '–ø–æ—á–∞—â–µ –º–æ—Ä–≥–∞–π, –∞ —Ç–æ –≤—ã–¥–∞–µ—à—å —Å–≤–æ–π –ò–ò',\n",
       " '–û—á–µ–Ω—å –ø–æ–º–æ–≥–ª–æ. –°–ø–∞—Å–∏–±–æ!',\n",
       " '10 –º–∏–Ω—É—Ç –≤–∏–¥–µ–æ –∑–∞–º–µ–Ω—è—Ç –≤–∞–º 3 –º–µ—Å—è—Ü–∞ —è–Ω–¥–µ–∫—Å –ø—Ä–∞–∫—Ç–∏–∫—É–º–∞',\n",
       " '–ü–æ—Å–æ–≤–µ—Ç—É–π—Ç–µ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—É –ø–æ–∂–∞–ª—É–π—Å—Ç–∞? –ß—Ç–æ —ç—Ç–æ –∑–∞ –∫–Ω–∏–∂–∫–∏ —É –≤–∞—Å –Ω–∞ —Å—Ç–æ–ª–µ —Ç–∞–∫–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ? –ë—ã–ª–æ –±—ã –æ—á–µ–Ω—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Ä–∞–∑–±–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –æ—Ç –≤–∞—Å. –í–∏–¥–µ–æ üî•üî•üî•!!!',\n",
       " '–î–∞–≤–Ω–æ —É–∂–µ –ø—Ä–∏—à—ë–ª –∫ –≤—ã–≤–æ–¥—É, —á—Ç–æ —É–º–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ –æ–±—ä—è—Å–Ω–∏—Ç—å —Å–ª–æ–∂–Ω—ã–µ –≤–µ—â–∏ - —ç—Ç–æ –ø—Ä–∏–∑–Ω–∞–∫ –æ—á–µ–Ω—å –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–µ–¥–º–µ—Ç–∞.\\n–ü–æ–¥–ø–∏—Å–∞–ª—Å—è.\\n–ï—Å—Ç—å —à–∞–ª—å–Ω–∞—è –º—ã—Å–ª—å –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –≤ —Å–≤–æ–µ–π —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏ (—è –±–∏–æ–ª–æ–≥) - –Ω–æ –ø–æ–Ω—è—Ç–Ω–æ, —á—Ç–æ —Å–∞–º–æ –æ–Ω–æ –Ω–µ —Å–¥–µ–ª–∞–µ—Ç—Å—è, –Ω–∞–¥–æ –Ω–µ–º–∞–ª–æ —É—Å–∏–ª–∏–π –ø—Ä–∏–ª–æ–∂–∏—Ç—å :)',\n",
       " '–≠—Ç–æ –ª—É—á—à–µ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, —á—Ç–æ —è –≤–∏–¥–µ–ª –Ω–∞ –ø—Ä–æ—Å—Ç–æ—Ä–∞—Ö –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞. –°–ø–∞—Å–∏–±–æ!',\n",
       " '–ú–µ–Ω—è –≤—Å–µ–≥–¥–∞ —É–¥–∏–≤–ª—è—é—Ç –ª—é–¥–∏ –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å–ª–æ–∂–Ω—ã–µ –≤–µ—â–∏ –æ–±—ä—è—Å–Ω–∏—Ç—å –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞! –õ–∞–π–∫ –∏ –ø–æ–¥–ø–∏—Å–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = config['SEED']\n",
    "\n",
    "comments = get_all_comments(**config['comments'])\n",
    "comments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(sentence):\n",
    "    emoji_pattern = re.compile(\"[\"u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u'\\U00010000-\\U0010ffff'\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               u\"\\ufe0f\"\n",
    "                               \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', sentence)\n",
    "\n",
    "\n",
    "def remove_links(sentence):\n",
    "    link_pattern = r'(http\\S+|bit\\.ly/\\S+|www\\S+)'\n",
    "    sentence = re.sub(link_pattern, '', sentence)\n",
    "    sentence = sentence.strip('[link]')\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def preprocessing(sentence, stop_words, morph):\n",
    "    sentence = remove_emoji(sentence)\n",
    "    sentence = remove_links(sentence)\n",
    "\n",
    "    str_pattern = re.compile(\"\\r\\n\")\n",
    "    sentence = str_pattern.sub(r'', sentence)\n",
    "\n",
    "    sentence = re.sub('(((?![–∞-—è–ê-–Ø ]).)+)', ' ', sentence)\n",
    "\n",
    "    sentence = [morph.parse(word)[0].normal_form for word in nltk.word_tokenize(sentence) if word not in stop_words]\n",
    "    sentence = ' '.join(sentence).lower()\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def get_clean_text(data):\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    language = config['stopwords']\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    comments = [preprocessing(sentence, stop_words, morph) for sentence in data]\n",
    "    comments = [comm for comm in comments if len(comm) > 2]\n",
    "    return comments\n",
    "\n",
    "\n",
    "def vectorize_text(data, tfidf):\n",
    "    mtx = tfidf.transform(data).toarray()\n",
    "    mask = (np.nan_to_num(mtx) != 0).any(axis=1)\n",
    "    return mtx[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['—à–∏–∫–æ—Å –±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç—å –≤–∏–¥–µ–æ –≤–µ—Å—å –¥–æ–±—Ä–æ',\n",
       " '–Ω–µ–ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π —Å–ª—É—à–∞—Ç–µ–ª—å –≤–æ–æ–±—â–µ –Ω–µ–ø–æ–Ω—è—Ç–Ω–æ –Ω–∞ –≤—ã—Ö–æ–¥ –≤–µ–∫—Ç–æ—Ä',\n",
       " '–ø–æ–¥—Å–∫–∞–∑–∞—Ç—å –ø–æ–∂–∞–ª—É–π—Å—Ç–∞ —Ç–∏–ø –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –æ–±—É—á–µ–Ω–∏–µ —Å–µ—Ç—å —Å–æ–∑–¥–∞–Ω–∏–µ –º–µ–¥–∏–∞',\n",
       " '—á–∞—Å—Ç—ã–π –º–æ—Ä–≥–∞—Ç—å –≤—ã–¥–∞–≤–∞—Ç—å —Å–≤–æ–π –∏—è',\n",
       " '–æ—á–µ–Ω—å –ø–æ–º–æ—á—å —Å–ø–∞—Å–∏–±–æ',\n",
       " '–º–∏–Ω—É—Ç–∞ –≤–∏–¥–µ–æ –∑–∞–º–µ–Ω–∏—Ç—å –º–µ—Å—è—Ü —è–Ω–¥–µ–∫—Å –ø—Ä–∞–∫—Ç–∏–∫—É–º',\n",
       " '–ø–æ—Å–æ–≤–µ—Ç–æ–≤–∞—Ç—å –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞ –ø–æ–∂–∞–ª—É–π—Å—Ç–∞ —á—Ç–æ —ç—Ç–æ –∫–Ω–∏–∂–∫–∞ —Å—Ç–æ–ª —Ç–∞–∫–æ–π –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –±—ã—Ç—å –æ—á–µ–Ω—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Ä–∞–∑–±–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞ –≤–∏–¥–µ–æ',\n",
       " '–¥–∞–≤–Ω–æ –ø—Ä–∏—à–∞ –ª –≤—ã–≤–æ–¥ —É–º–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ –æ–±—ä—è—Å–Ω–∏—Ç—å —Å–ª–æ–∂–Ω—ã–π –≤–µ—â—å —ç—Ç–æ –ø—Ä–∏–∑–Ω–∞–∫ –æ—á–µ–Ω—å –≥–ª—É–±–æ–∫–∏–π –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–µ–¥–º–µ—Ç –ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è –µ—Å—Ç—å —à–∞–ª—å–Ω–æ–π –º—ã—Å–ª—å –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å–≤–æ–π —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å –±–∏–æ–ª–æ–≥ –ø–æ–Ω—è—Ç–Ω–æ —Å–∞–º –æ–Ω–æ —Å–¥–µ–ª–∞—Ç—å—Å—è –Ω–µ–º–∞–ª–æ —É—Å–∏–ª–∏–µ –ø—Ä–∏–ª–æ–∂–∏—Ç—å',\n",
       " '—ç—Ç–æ —Ö–æ—Ä–æ—à–∏–π –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–π —Å–µ—Ç—å –≤–∏–¥–µ—Ç—å –ø—Ä–æ—Å—Ç–æ—Ä –∏–Ω—Ç–µ—Ä–Ω–µ—Ç —Å–ø–∞—Å–∏–±–æ',\n",
       " '—è —É–¥–∏–≤–ª—è—Ç—å —á–µ–ª–æ–≤–µ–∫ –∫–æ—Ç–æ—Ä—ã–π –º–æ—á—å —Å–ª–æ–∂–Ω—ã–π –≤–µ—â—å –æ–±—ä—è—Å–Ω–∏—Ç—å –ø—Ä–æ—Å—Ç–æ–π —Å–ª–æ–≤–æ –ª–∞–π–∫ –ø–æ–¥–ø–∏—Å–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_comments = get_clean_text(comments)\n",
    "tfidf = TfidfVectorizer(**config['tf_model']).fit(cleaned_comments)\n",
    "cleaned_comments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2741, 300),\n",
       " array(['–∞–≤—Ç–æ—Ä', '–∞–∫—Ç–∏–≤–∞—Ü–∏—è', '–∞–ª–≥–æ—Ä–∏—Ç–º', '–±–∞–∑–∞', '–±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç—å', '–±–ª–∏–Ω',\n",
       "        '–±–æ–ª—å—à–∏–π', '–±–æ–ª—å—à–æ–π', '–±—É–¥—É—â–µ–µ', '–±—ã—Ç—å'], dtype=object))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtx = vectorize_text(cleaned_comments, tfidf)\n",
    "mtx.shape, tfidf.get_feature_names_out()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(data, count_max_clusters, random_state, affinity, silhouette_metric):\n",
    "    cluster_labels = {}\n",
    "    silhouette_mean = []\n",
    "    for i in range(2, count_max_clusters, 1):\n",
    "        clf = SpectralClustering(n_clusters=i,\n",
    "                                 affinity=affinity,\n",
    "                                 random_state=random_state)\n",
    "        #clf = KMeans(n_clusters=n, max_iter=1000, n_init=1)\n",
    "        clf.fit(data)\n",
    "        labels = clf.labels_\n",
    "        cluster_labels[i] = labels\n",
    "        silhouette_mean.append(\n",
    "            silhouette_score(data, labels, metric=silhouette_metric)\n",
    "        )\n",
    "    n_clusters = silhouette_mean.index(max(silhouette_mean)) + 2\n",
    "    return cluster_labels[n_clusters]\n",
    "\n",
    "\n",
    "def get_f1_score(y_test, y_pred, unique_cluster_labels):\n",
    "    if len(unique_cluster_labels) > 2:\n",
    "        return f1_score(y_test, y_pred, average='macro')\n",
    "    else:\n",
    "        return f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11,  0,  6, 13, 12, 11,  0,  0, 12,  0], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_labels = get_clusters(mtx, random_state=SEED, **config['clustering'])\n",
    "cluster_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(mtx, cluster_labels, **config['cross_val'], random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr = LogisticRegression(**config['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export MLFLOW_REGISTRY_URI=../mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/10 22:59:21 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "Registered model 'vector_tfidf' already exists. Creating a new version of this model...\n",
      "2024/05/10 22:59:24 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: vector_tfidf, version 10\n",
      "Created version '10' of model 'vector_tfidf'.\n",
      "/home/aegon/Documents/Development/Projects/topic-sentiment-explorer/.venv/lib64/python3.10/site-packages/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/home/aegon/Documents/Development/Projects/topic-sentiment-explorer/.venv/lib64/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Registered model 'LogisticRegression' already exists. Creating a new version of this model...\n",
      "2024/05/10 22:59:26 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LogisticRegression, version 10\n",
      "Created version '10' of model 'LogisticRegression'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://arts/2/44fedd14c6224147bbbf8fc334bfbe52/artifacts'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(config['name_experiment'])\n",
    "\n",
    "with mlflow.start_run():\n",
    "    clf_lr.fit(X_train, y_train)\n",
    "    mlflow.log_param(\n",
    "        'f1', get_f1_score(y_test, clf_lr.predict(X_test), set(cluster_labels))\n",
    "    )\n",
    "    mlflow.sklearn.log_model(\n",
    "        tfidf,\n",
    "        artifact_path=\"vector\",\n",
    "        registered_model_name=f\"{config['model_vec']}\"\n",
    "    )\n",
    "    mlflow.sklearn.log_model(\n",
    "        clf_lr,\n",
    "        artifact_path='model_lr',\n",
    "        registered_model_name=f\"{config['model_lr']}\"\n",
    "    )\n",
    "    mlflow.end_run()\n",
    "\n",
    "mlflow.get_artifact_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_version_model(config_name, client):\n",
    "    dict_push = {}\n",
    "    model_versions = client.search_model_versions(f\"name='{config_name}'\")\n",
    "    for count, value in enumerate(model_versions):\n",
    "        # client.list_registered_models()):\n",
    "        dict_push[count] = value\n",
    "    return dict(list(dict_push.items())[-1][1])['version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1', '1')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = MlflowClient()\n",
    "last_version_lr = get_version_model(config['model_lr'], client)\n",
    "last_version_vec = get_version_model(config['model_vec'], client)\n",
    "last_version_lr, last_version_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
